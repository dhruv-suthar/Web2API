# Web2API âœ¦

> A data gathering platform that turns any website into a **type-safe API** â€” with schema validation, AI-powered extraction, and scheduled monitoring.

## âœ¨ Features

- ğŸ¤– **AI-Powered Extraction** - Define your data in JSON schema, GPT-4o-mini extracts it
- ğŸš€ **Zero Selectors** - No CSS, XPath, or regex - AI understands the page semantically
- âš¡ **Two-Level Caching** - Extraction cache + content cache = instant repeat requests
- ğŸ“Š **Real-time Progress** - WebSocket streaming shows extraction status live
- ğŸ”„ **Scheduled Monitoring** - Cron jobs for automatic re-scraping
- ğŸ› ï¸ **Visual Workbench** - See your workflow in Motia's flow visualization


## ğŸ—ºï¸ Roadmap

### Core Features

| Feature | Status | Description |
|---------|--------|-------------|
| JSON Schema Extraction | âœ… Done | Define output structure, AI extracts matching data |
| Firecrawl Integration | âœ… Done | Handles JS rendering, anti-bot protection |
| GPT-4o-mini Extraction | âœ… Done | Smart extraction without brittle selectors |
| Extraction Caching | âœ… Done | Cache by URL + schema hash |
| Content Caching | âœ… Done | Cache raw content by URL only |
| Real-time Progress | âœ… Done | WebSocket updates via Motia Streams |
| Webhook Notifications | ğŸ”œ Planned | Notify on data changes |

### API

| Feature | Status | Description |
|---------|--------|-------------|
| Create Scraper | âœ… Done | Define name, schema, options |
| Run Scraper | âœ… Done | Execute against any URL |
| Get Status | âœ… Done | Poll job progress |
| Get Results | âœ… Done | Fetch extracted data |
| List Scrapers | âœ… Done | View all scrapers |
| Delete Monitor | âœ… Done | Remove scheduled jobs |

### Scheduled Monitoring

| Feature | Status | Description |
|---------|--------|-------------|
| Cron Scheduler | âœ… Done | Check monitors every 5 minutes |
| Auto-Monitoring | âœ… Done | URLs auto-added to monitoring |
| Fresh Scrapes | âœ… Done | Bypass cache for scheduled runs |
| Webhook Notifications | ğŸ”œ Planned | Notify on data changes |

### Frontend

| Feature | Status | Description |
|---------|--------|-------------|
| Scraper Dashboard | âœ… Done | List and manage scrapers |
| Create Scraper Form | âœ… Done | Visual schema builder |
| Run Scraper UI | âœ… Done | Execute with options |
| Job Progress | âœ… Done | Real-time status updates |
| Results Viewer | âœ… Done | Pretty JSON display with syntax highlighting |
| API Usage Modal | âœ… Done | Show how to use scrapers programmatically |

**Want to contribute?** PRs welcome!

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+
- Python 3.9+
- API Keys: Firecrawl, OpenAI

### Installation

```bash
git clone https://github.com/MotiaDev/web2api.git
cd web2api

# Backend
cd web2api-backend
npm install

# Frontend
cd ../web2api-frontend
npm install
```

### Configure API Keys

Create `.env` in `web2api-backend/`:

```bash
FIRECRAWL_API_KEY=fc-xxxxxxxxxxxx
OPENAI_API_KEY=sk-xxxxxxxxxxxx
```

Create `.env.local` in `web2api-frontend/`:

```bash
NEXT_PUBLIC_API_URL=http://localhost:3001
```

### Start Development

```bash
# Terminal 1: Start Motia backend
cd web2api-backend
npm run dev

# Terminal 2: Start Next.js frontend
cd web2api-frontend
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) and create your first scraper!

## ğŸ“Š How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         WEB2API FLOW                             â”‚
â”‚                                                                  â”‚
â”‚  1. Define scraper with JSON schema                              â”‚
â”‚     {"news_titles": ["string"]}                                  â”‚
â”‚                                                                  â”‚
â”‚  2. POST /scrape/:id with target URL                             â”‚
â”‚     â””â”€â–¶ API Step emits to queue                                  â”‚
â”‚                                                                  â”‚
â”‚  3. FetchWebpage (Event Step)                                    â”‚
â”‚     â”œâ”€â–¶ Check extraction cache â†’ HIT â†’ Return instantly          â”‚
â”‚     â”œâ”€â–¶ Check content cache â†’ HIT â†’ Skip Firecrawl               â”‚
â”‚     â””â”€â–¶ MISS â†’ Scrape with Firecrawl                             â”‚
â”‚                                                                  â”‚
â”‚  4. ExtractWithLLM (Event Step)                                  â”‚
â”‚     â””â”€â–¶ GPT-4o-mini extracts structured data                     â”‚
â”‚                                                                  â”‚
â”‚  5. StoreResults (Event Step)                                    â”‚
â”‚     â”œâ”€â–¶ Validate against schema                                  â”‚
â”‚     â”œâ”€â–¶ Cache for future requests                                â”‚
â”‚     â””â”€â–¶ Update job status                                        â”‚
â”‚                                                                  â”‚
â”‚  6. Receive clean JSON matching your schema                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Motia Workbench

Visualize and test your API flows in the Motia Workbench:
<img width="1871" height="703" alt="Screenshot 2025-12-21 at 9 25 58â€¯PM" src="https://github.com/user-attachments/assets/65889357-47e8-4341-8fab-35230c2f170a" />

<img width="1886" height="700" alt="Screenshot 2025-12-21 at 9 25 16â€¯PM" src="https://github.com/user-attachments/assets/1a76d8cd-b857-44eb-83e3-9486cc5a2168" />

<img width="1909" height="741" alt="Screenshot 2025-12-21 at 9 29 43â€¯PM" src="https://github.com/user-attachments/assets/488248c3-ef14-4613-96d4-2f9bf81bb1f4" />





### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| ` /scrapers` | POST | Create scraper with JSON schema |
| ` /scrapers` | GET | List all scrapers |
| ` /scrapers/:id` | GET | Get scraper details |
| ` /scrape/:scraperId` | POST | Execute scraper on URL |
| ` /status/:jobId` | GET | Poll job status |
| ` /results/:jobId` | GET | Get extraction results |
| ` /monitors` | GET | List scheduled monitors |
| ` /monitors/:id` | DELETE | Remove monitor |

### Example Usage

```bash
# Create a scraper
curl -X POST http://localhost:3001 /scrapers \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Hacker News Scraper",
    "schema": {"news_titles": ["string"]},
    "example_url": "https://news.ycombinator.com/"
  }'

# Run the scraper
curl -X POST http://localhost:3001 /scrape/:scraperId \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://news.ycombinator.com/",
    "options": {"use_cache": true}
  }'
```

## ğŸš¢ Deployment

### Backend â†’ Railway

```bash
cd web2api-backend
railway up
railway domain  # Get your URL
```

### Frontend â†’ Vercel

```bash
cd web2api-frontend
vercel --prod
```

Set environment variables in Vercel:
- `NEXT_PUBLIC_API_URL` - Your Railway backend URL (e.g., `https://web2api.up.railway.app `)

## ğŸ“ Project Structure

```
web2api/
â”œâ”€â”€ web2api-backend/
â”‚   â”œâ”€â”€ steps/
â”‚   â”‚   â”œâ”€â”€ api/              # 9 API Steps (Python)
â”‚   â”‚   â”‚   â”œâ”€â”€ create_scraper_step.py
â”‚   â”‚   â”‚   â”œâ”€â”€ run_scraper_step.py
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ events/           # 4 Event Steps (Python)
â”‚   â”‚   â”‚   â”œâ”€â”€ fetch_webpage_step.py
â”‚   â”‚   â”‚   â”œâ”€â”€ extract_with_llm_step.py
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ cron/             # 1 Cron Step (Python)
â”‚   â”‚   â”‚   â””â”€â”€ run_scheduled_monitors_step.py
â”‚   â”‚   â””â”€â”€ streams/          # 1 Stream (Python)
â”‚   â”‚       â””â”€â”€ job_progress_stream.py
â”‚   â”œâ”€â”€ src/services/         # DDD Service Layer
â”‚   â””â”€â”€ motia.config.ts
â”‚
â”œâ”€â”€ web2api-frontend/
â”‚   â”œâ”€â”€ app/                  # Next.js App Router
â”‚   â”œâ”€â”€ components/           # React Components
â”‚   â””â”€â”€ lib .ts            # API Client
```

## ğŸ› ï¸ Tech Stack

- **Backend**: [Motia](https://motia.dev) - Event-driven API framework
- **Backend Language**: Python 3.9
- **Frontend**: Next.js 16 + React 19
- **Styling**: Tailwind CSS 4
- **Scraping**: Firecrawl
- **AI Extraction**: OpenAI GPT-4o-mini

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| `Property 'X' does not exist on type 'Handlers'` | Run `npx motia generate-types` |
| Firecrawl rate limit | Add delay between requests |
| OpenAI timeout | Increase handler timeout in infrastructure config |
| CORS errors | Ensure `NEXT_PUBLIC_API_URL` is correct |

## ğŸ† Backend Reloaded Hackathon

Built for the [Backend Reloaded Hackathon](https://www.wemakedevs.org/hackathons/motiahack25) by [WeMakeDevs](https://www.wemakedevs.org/).

### How Web2API Uses Motia

| Motia Feature | Implementation |
|---------------|----------------|
| **API Steps** | 9 REST endpoints for scraper CRUD and job management |
| **Event Steps** | Async pipeline: Fetch â†’ Extract â†’ Store |
| **Cron Steps** | Scheduled monitoring every 5 minutes |
| **State Management** | Redis-backed caching with two cache layers |


## ğŸ“š Learn More

- [Motia Documentation](https://www.motia.dev/docs)
- [Motia GitHub](https://github.com/MotiaDev/motia)
- [Firecrawl Docs](https://firecrawl.dev)
- [OpenAI API](https://platform.openai.com/docs)


